{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Massive Text Data\n",
    "\n",
    "**TA Session - Text and NLP for Data Science**  \n",
    "Barcelona School of Economics\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem\n",
    "\n",
    "You have a 16GB CSV of news articles. You need articles from March 2023 only.\n",
    "\n",
    "Your laptop has 16GB of RAM.\n",
    "\n",
    "What do you do?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Naive Approach (Watch It Fail)\n",
    "\n",
    "Let's try what every pandas user does first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Check our environment\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"Your machine has {ram_gb:.1f} GB of RAM\")\n",
    "\n",
    "data_file = \"../data/cc_news_large.csv\"\n",
    "file_size_gb = os.path.getsize(data_file) / (1024**3)\n",
    "print(f\"The data file is {file_size_gb:.1f} GB\")\n",
    "print()\n",
    "print(f\"Ratio: file is {file_size_gb/ram_gb*100:.0f}% of your RAM\")\n",
    "print(\"Pandas typically needs 2-5x the file size in memory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ WARNING: This cell WILL crash or hang on most machines.\n",
    "# That's the point. Run it, wait 30-60 seconds, then interrupt the kernel.\n",
    "#\n",
    "# Watch your RAM usage (Activity Monitor on Mac, Task Manager on Windows)\n",
    "# as this runs. You'll see it spike and then... bad things happen.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The naive approach - just load everything\n",
    "df = pd.read_csv(\"../data/cc_news_large.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "If you're lucky: `MemoryError`\n",
    "\n",
    "If you're unlucky: Your laptop started swapping to disk, became unresponsive, and you had to force-kill Python.\n",
    "\n",
    "**Why?** Pandas loads the ENTIRE file into memory before you can do anything with it. A 10GB CSV file typically needs 20-30GB of RAM to load into a DataFrame (due to Python object overhead, string storage, index creation, etc.).\n",
    "\n",
    "Your laptop doesn't have that. So pandas tries to use disk as overflow memory (swapping), which is ~1000x slower than RAM, and everything grinds to a halt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If for whatever reason you were able to load it, you could filter like this... se how long it takes.\n",
    "df[df[\"date\"].str.startswith(\"2023-03\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the dataframe to free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Old Way - Chunked Reading\n",
    "\n",
    "Before modern tools existed, we had to do this manually. It's educational to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# The chunked approach: read the file in pieces\n",
    "CHUNK_SIZE = 50_000  # rows per chunk\n",
    "TARGET_START = \"2023-03-01\"\n",
    "TARGET_END = \"2023-04-01\"\n",
    "\n",
    "start_time = time.time()\n",
    "matching_chunks = []\n",
    "total_rows_scanned = 0\n",
    "\n",
    "print(f\"Scanning file in chunks of {CHUNK_SIZE:,} rows...\")\n",
    "print(f\"Looking for dates between {TARGET_START} and {TARGET_END}\")\n",
    "print()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\"../data/cc_news_large.csv\", chunksize=CHUNK_SIZE)):\n",
    "    total_rows_scanned += len(chunk)\n",
    "    \n",
    "    # Filter this chunk for our date range\n",
    "    mask = (chunk['date'] >= TARGET_START) & (chunk['date'] < TARGET_END)\n",
    "    matching = chunk[mask]\n",
    "    \n",
    "    if len(matching) > 0:\n",
    "        matching_chunks.append(matching)\n",
    "    \n",
    "    # Progress update\n",
    "    if (i + 1) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Chunk {i+1}: scanned {total_rows_scanned:,} rows, \"\n",
    "              f\"found {sum(len(c) for c in matching_chunks):,} matches, \"\n",
    "              f\"elapsed {elapsed:.1f}s\")\n",
    "\n",
    "# Combine all matching chunks\n",
    "if matching_chunks:\n",
    "    result = pd.concat(matching_chunks, ignore_index=True)\n",
    "else:\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print()\n",
    "print(f\"Done! Found {len(result):,} articles from March 2023\")\n",
    "print(f\"Total time: {elapsed:.1f} seconds\")\n",
    "print(f\"Scanned {total_rows_scanned:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we got\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much RAM are we using now?\n",
    "ram_used_gb = psutil.virtual_memory().used / (1024**3)\n",
    "print(f\"RAM used at this point in time: {ram_used_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chunked Approach: Analysis\n",
    "\n",
    "**Pros:**\n",
    "- It works! Doesn't crash.\n",
    "- Memory stays bounded (only one chunk in RAM at a time)\n",
    "\n",
    "**Cons:**\n",
    "- Slow: must scan the entire file even if target data is at the end\n",
    "- Tedious: you're writing custom iteration logic\n",
    "- Error-prone: easy to mess up the chunk boundary handling\n",
    "- Doesn't scale: what if you need to do this 10 times with different filters?\n",
    "\n",
    "**The deeper problem:** You're essentially writing a half-baked database query engine by hand. Someone already did this work properly. Let's use their tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Modern Way\n",
    "\n",
    "Two tools that solve this properly: **DuckDB** and **Polars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Polars\n",
    "\n",
    "Polars is a DataFrame library like pandas, but designed for speed and memory efficiency.\n",
    "\n",
    "The key feature: **lazy evaluation**. You build up a query plan, and Polars optimizes and executes it only when you call `.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# scan_csv (not read_csv!) creates a lazy query\n",
    "result_polars = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .collect()  # Execute the query\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Polars: Found {len(result_polars):,} articles in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_polars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lazy Evaluation\n",
    "\n",
    "Watch what happens when you DON'T call `.collect()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns INSTANTLY - no data is read yet!\n",
    "lazy_query = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .select([\"date\", \"title\", \"domain\"])\n",
    ")\n",
    "\n",
    "print(\"Type:\", type(lazy_query))\n",
    "print()\n",
    "print(\"Query plan:\")\n",
    "print(lazy_query.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars shows you the execution plan. Notice how it pushes the filter down and only reads the columns you selected. This is query optimization - the same thing databases do.\n",
    "\n",
    "Only when you call `.collect()` does the actual work happen.\n",
    "\n",
    "> The main difference with read_csv is that this one would get the whole thing into memory to filter, instead of just bringing to memory what its actually needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex Polars example: aggregation with word counting\n",
    "articles_per_day = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .with_columns([\n",
    "        pl.col(\"text\").str.split(\" \").list.len().alias(\"word_count\")\n",
    "    ])\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"article_count\"),\n",
    "        pl.col(\"word_count\").mean().alias(\"avg_words\")\n",
    "    ])\n",
    "    .sort(\"date\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "articles_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: DuckDB\n",
    "\n",
    "DuckDB is an embedded analytical database. It's like SQLite, a simple small file-based database system, but designed for data analysis instead of transactions.\n",
    "\n",
    "The magic: it can run SQL queries directly on CSV/Parquet files without loading them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# One line. That's it.\n",
    "result_duckdb = duckdb.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"DuckDB: Found {len(result_duckdb):,} articles in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_duckdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result_duckdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is DuckDB fast?\n",
    "\n",
    "1. **Streaming execution**: It doesn't load the whole file. It streams through it, filtering as it goes.\n",
    "2. **Columnar processing**: It can skip columns you don't need entirely.\n",
    "3. **Vectorized operations**: Processes data in batches, not row-by-row.\n",
    "4. **Predicate pushdown**: The WHERE clause is applied during reading, not after.\n",
    "\n",
    "You wrote one SQL query. DuckDB figured out the optimal execution strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More DuckDB Examples\n",
    "\n",
    "You can do complex analytics without ever loading the full file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count articles per domain, across the ENTIRE 16GB file\n",
    "domain_counts = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        domain,\n",
    "        COUNT(*) as article_count,\n",
    "        AVG(LENGTH(text)) as avg_text_length\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    GROUP BY domain\n",
    "    ORDER BY article_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").df()\n",
    "\n",
    "domain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find articles containing specific keywords\n",
    "inflation_articles = duckdb.sql(\"\"\"\n",
    "    SELECT date, title, domain, LENGTH(text) as text_length\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE text ILIKE '%inflation%'\n",
    "    AND date >= '2023-01-01'\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "inflation_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Comparison Summary\n",
    "\n",
    "| Approach | Time | Memory | Code Complexity |\n",
    "|----------|------|--------|----------------|\n",
    "| Naive pandas | ❌ Crashes | ❌ Exceeds RAM | Simple (but useless) |\n",
    "| Chunked pandas | Slow | ✓ Bounded | Complex (manual) |\n",
    "| DuckDB | Fast | ✓ Efficient | Simple (SQL) |\n",
    "| Polars | Mid | ✓ Efficient | Simple (fluent API) |\n",
    "\n",
    "### When to use what?\n",
    "\n",
    "**DuckDB:**\n",
    "- You know SQL\n",
    "- One-off analytics queries\n",
    "- Joining multiple large files\n",
    "- Working with Parquet files (even faster)\n",
    "\n",
    "**Polars:**\n",
    "- You prefer Python-native APIs\n",
    "- Building data pipelines\n",
    "- Need pandas-like operations but faster\n",
    "\n",
    "**Chunked pandas:**\n",
    "- Legacy code you can't rewrite\n",
    "- Very specific streaming requirements\n",
    "- Otherwise: just don't. Use DuckDB or Polars.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Converting to Parquet\n",
    "\n",
    "If you'll work with this data repeatedly, convert it to Parquet format. Parquet is:\n",
    "- Columnar (only read columns you need)\n",
    "- Compressed (16GB CSV → ~2GB Parquet)\n",
    "- Much faster to read\n",
    "\n",
    "DuckDB can do this conversion without loading the full file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV to Parquet (run once, benefit forever)\n",
    "# It takes a couple of minutes...\n",
    "\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (SELECT * FROM read_csv_auto('../data/cc_news_large.csv'))\n",
    "    TO '../data/cc_news_large.parquet' (FORMAT PARQUET, COMPRESSION ZSTD)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then queries on Parquet are even faster (half the time):\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM '../data/cc_news_large.parquet'\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "    SELECT date\n",
    "    FROM '../data/cc_news_large.parquet'\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Pandas is not a database.** It loads everything into memory. For files bigger than ~1/3 of your RAM, it will struggle or fail.\n",
    "\n",
    "2. **The \"big data\" threshold is lower than you think.** On a 16GB laptop, a 5GB file is already problematic for pandas.\n",
    "\n",
    "3. **Modern tools make \"hard\" problems trivial.** DuckDB and Polars handle large files elegantly. Learn them.\n",
    "\n",
    "4. **Lazy evaluation is a superpower.** Building a query plan before executing means the tool can optimize. You don't have to.\n",
    "\n",
    "5. **When in doubt, use SQL.** DuckDB lets you write SQL against CSV files. If you know SQL, you already know how to handle big files.\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Exercise\n",
    "\n",
    "Open `02_exercise.ipynb` to apply what you've learned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-ta-massive-text (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
