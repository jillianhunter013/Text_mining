{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Massive Text Data\n",
    "\n",
    "**TA Session - Text and NLP for Data Science**  \n",
    "Barcelona School of Economics\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem\n",
    "\n",
    "You have a 16GB CSV of news articles. You need articles from March 2023 only.\n",
    "\n",
    "Your laptop has 16GB of RAM.\n",
    "\n",
    "What do you do?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Naive Approach (Watch It Fail)\n",
    "\n",
    "Let's try what every pandas user does first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your machine has 8.0 GB of RAM\n",
      "The data file is 0.7 GB\n",
      "\n",
      "Ratio: file is 9% of your RAM\n",
      "Pandas typically needs 2-5x the file size in memory...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Check our environment\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"Your machine has {ram_gb:.1f} GB of RAM\")\n",
    "\n",
    "data_file = \"../data/cc_news_large.csv\"\n",
    "file_size_gb = os.path.getsize(data_file) / (1024**3)\n",
    "print(f\"The data file is {file_size_gb:.1f} GB\")\n",
    "print()\n",
    "print(f\"Ratio: file is {file_size_gb/ram_gb*100:.0f}% of your RAM\")\n",
    "print(\"Pandas typically needs 2-5x the file size in memory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ⚠️ WARNING: This cell WILL crash or hang on most machines.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# That's the point. Run it, wait 30-60 seconds, then interrupt the kernel.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Watch your RAM usage (Activity Monitor on Mac, Task Manager on Windows)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# as this runs. You'll see it spike and then... bad things happen.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# The naive approach - just load everything\u001b[39;00m\n\u001b[32m     10\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/cc_news_large.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Text_mining/Session3/TA/.venv/lib/python3.12/site-packages/pandas/__init__.py:22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m     24\u001b[39m     )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     26\u001b[39m     _module = _err.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Text_mining/Session3/TA/.venv/lib/python3.12/site-packages/pandas/compat/__init__.py:28\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     CHAINED_WARNING_DISABLED,\n\u001b[32m     20\u001b[39m     IS64,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     WASM,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     HAS_PYARROW,\n\u001b[32m     30\u001b[39m     PYARROW_MIN_VERSION,\n\u001b[32m     31\u001b[39m     pa_version_under14p0,\n\u001b[32m     32\u001b[39m     pa_version_under14p1,\n\u001b[32m     33\u001b[39m     pa_version_under16p0,\n\u001b[32m     34\u001b[39m     pa_version_under17p0,\n\u001b[32m     35\u001b[39m     pa_version_under18p0,\n\u001b[32m     36\u001b[39m     pa_version_under19p0,\n\u001b[32m     37\u001b[39m     pa_version_under20p0,\n\u001b[32m     38\u001b[39m     pa_version_under21p0,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Text_mining/Session3/TA/.venv/lib/python3.12/site-packages/pandas/compat/pyarrow.py:9\u001b[39m\n\u001b[32m      7\u001b[39m PYARROW_MIN_VERSION = \u001b[33m\"\u001b[39m\u001b[33m13.0.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     11\u001b[39m     _palv = Version(Version(pa.__version__).base_version)\n\u001b[32m     12\u001b[39m     pa_version_under14p0 = _palv < Version(\u001b[33m\"\u001b[39m\u001b[33m14.0.0\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Text_mining/Session3/TA/.venv/lib/python3.12/site-packages/pyarrow/__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     57\u001b[39m         __version__ = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BuildInfo, CppBuildInfo, RuntimeInfo, set_timezone_db_path,\n\u001b[32m     60\u001b[39m                          MonthDayNano, VersionInfo, build_info, cpp_build_info,\n\u001b[32m     61\u001b[39m                          cpp_version, cpp_version_info, runtime_info,\n\u001b[32m     62\u001b[39m                          cpu_count, set_cpu_count, enable_signal_handlers,\n\u001b[32m     63\u001b[39m                          io_thread_count, set_io_thread_count)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_versions\u001b[39m():\n\u001b[32m     67\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m    Print various version information, to help with error reporting.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ⚠️ WARNING: This cell WILL crash or hang on most machines.\n",
    "# That's the point. Run it, wait 30-60 seconds, then interrupt the kernel.\n",
    "#\n",
    "# Watch your RAM usage (Activity Monitor on Mac, Task Manager on Windows)\n",
    "# as this runs. You'll see it spike and then... bad things happen.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The naive approach - just load everything\n",
    "df = pd.read_csv(\"../data/cc_news_large.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "If you're lucky: `MemoryError`\n",
    "\n",
    "If you're unlucky: Your laptop started swapping to disk, became unresponsive, and you had to force-kill Python.\n",
    "\n",
    "**Why?** Pandas loads the ENTIRE file into memory before you can do anything with it. A 10GB CSV file typically needs 20-30GB of RAM to load into a DataFrame (due to Python object overhead, string storage, index creation, etc.).\n",
    "\n",
    "Your laptop doesn't have that. So pandas tries to use disk as overflow memory (swapping), which is ~1000x slower than RAM, and everything grinds to a halt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# If for whatever reason you were able to load it, you could filter like this... se how long it takes.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].str.startswith(\u001b[33m\"\u001b[39m\u001b[33m2023-03\u001b[39m\u001b[33m\"\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# If for whatever reason you were able to load it, you could filter like this... se how long it takes.\n",
    "df[df[\"date\"].str.startswith(\"2023-03\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Delete the dataframe to free up memory\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mdf\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete the dataframe to free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Old Way - Chunked Reading\n",
    "\n",
    "Before modern tools existed, we had to do this manually. It's educational to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# The chunked approach: read the file in pieces\n",
    "CHUNK_SIZE = 50_000  # rows per chunk\n",
    "TARGET_START = \"2023-03-01\"\n",
    "TARGET_END = \"2023-04-01\"\n",
    "\n",
    "start_time = time.time()\n",
    "matching_chunks = []\n",
    "total_rows_scanned = 0\n",
    "\n",
    "print(f\"Scanning file in chunks of {CHUNK_SIZE:,} rows...\")\n",
    "print(f\"Looking for dates between {TARGET_START} and {TARGET_END}\")\n",
    "print()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\"../data/cc_news_large.csv\", chunksize=CHUNK_SIZE)):\n",
    "    total_rows_scanned += len(chunk)\n",
    "    \n",
    "    # Filter this chunk for our date range\n",
    "    mask = (chunk['date'] >= TARGET_START) & (chunk['date'] < TARGET_END)\n",
    "    matching = chunk[mask]\n",
    "    \n",
    "    if len(matching) > 0:\n",
    "        matching_chunks.append(matching)\n",
    "    \n",
    "    # Progress update\n",
    "    if (i + 1) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Chunk {i+1}: scanned {total_rows_scanned:,} rows, \"\n",
    "              f\"found {sum(len(c) for c in matching_chunks):,} matches, \"\n",
    "              f\"elapsed {elapsed:.1f}s\")\n",
    "\n",
    "# Combine all matching chunks\n",
    "if matching_chunks:\n",
    "    result = pd.concat(matching_chunks, ignore_index=True)\n",
    "else:\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print()\n",
    "print(f\"Done! Found {len(result):,} articles from March 2023\")\n",
    "print(f\"Total time: {elapsed:.1f} seconds\")\n",
    "print(f\"Scanned {total_rows_scanned:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we got\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much RAM are we using now?\n",
    "ram_used_gb = psutil.virtual_memory().used / (1024**3)\n",
    "print(f\"RAM used at this point in time: {ram_used_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chunked Approach: Analysis\n",
    "\n",
    "**Pros:**\n",
    "- It works! Doesn't crash.\n",
    "- Memory stays bounded (only one chunk in RAM at a time)\n",
    "\n",
    "**Cons:**\n",
    "- Slow: must scan the entire file even if target data is at the end\n",
    "- Tedious: you're writing custom iteration logic\n",
    "- Error-prone: easy to mess up the chunk boundary handling\n",
    "- Doesn't scale: what if you need to do this 10 times with different filters?\n",
    "\n",
    "**The deeper problem:** You're essentially writing a half-baked database query engine by hand. Someone already did this work properly. Let's use their tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Modern Way\n",
    "\n",
    "Two tools that solve this properly: **DuckDB** and **Polars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Polars\n",
    "\n",
    "Polars is a DataFrame library like pandas, but designed for speed and memory efficiency.\n",
    "\n",
    "The key feature: **lazy evaluation**. You build up a query plan, and Polars optimizes and executes it only when you call `.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# scan_csv (not read_csv!) creates a lazy query\n",
    "result_polars = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .collect()  # Execute the query\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Polars: Found {len(result_polars):,} articles in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_polars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lazy Evaluation\n",
    "\n",
    "Watch what happens when you DON'T call `.collect()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns INSTANTLY - no data is read yet!\n",
    "lazy_query = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .select([\"date\", \"title\", \"domain\"])\n",
    ")\n",
    "\n",
    "print(\"Type:\", type(lazy_query))\n",
    "print()\n",
    "print(\"Query plan:\")\n",
    "print(lazy_query.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars shows you the execution plan. Notice how it pushes the filter down and only reads the columns you selected. This is query optimization - the same thing databases do.\n",
    "\n",
    "Only when you call `.collect()` does the actual work happen.\n",
    "\n",
    "> The main difference with read_csv is that this one would get the whole thing into memory to filter, instead of just bringing to memory what its actually needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex Polars example: aggregation with word counting\n",
    "articles_per_day = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")\n",
    "    .filter(pl.col(\"date\") >= \"2023-03-01\")\n",
    "    .filter(pl.col(\"date\") < \"2023-04-01\")\n",
    "    .with_columns([\n",
    "        pl.col(\"text\").str.split(\" \").list.len().alias(\"word_count\")\n",
    "    ])\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"article_count\"),\n",
    "        pl.col(\"word_count\").mean().alias(\"avg_words\")\n",
    "    ])\n",
    "    .sort(\"date\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "articles_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: DuckDB\n",
    "\n",
    "DuckDB is an embedded analytical database. It's like SQLite, a simple small file-based database system, but designed for data analysis instead of transactions.\n",
    "\n",
    "The magic: it can run SQL queries directly on CSV/Parquet files without loading them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# One line. That's it.\n",
    "result_duckdb = duckdb.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"DuckDB: Found {len(result_duckdb):,} articles in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_duckdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result_duckdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is DuckDB fast?\n",
    "\n",
    "1. **Streaming execution**: It doesn't load the whole file. It streams through it, filtering as it goes.\n",
    "2. **Columnar processing**: It can skip columns you don't need entirely.\n",
    "3. **Vectorized operations**: Processes data in batches, not row-by-row.\n",
    "4. **Predicate pushdown**: The WHERE clause is applied during reading, not after.\n",
    "\n",
    "You wrote one SQL query. DuckDB figured out the optimal execution strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More DuckDB Examples\n",
    "\n",
    "You can do complex analytics without ever loading the full file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count articles per domain, across the ENTIRE 16GB file\n",
    "domain_counts = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        domain,\n",
    "        COUNT(*) as article_count,\n",
    "        AVG(LENGTH(text)) as avg_text_length\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    GROUP BY domain\n",
    "    ORDER BY article_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").df()\n",
    "\n",
    "domain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find articles containing specific keywords\n",
    "inflation_articles = duckdb.sql(\"\"\"\n",
    "    SELECT date, title, domain, LENGTH(text) as text_length\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE text ILIKE '%inflation%'\n",
    "    AND date >= '2023-01-01'\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "inflation_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Comparison Summary\n",
    "\n",
    "| Approach | Time | Memory | Code Complexity |\n",
    "|----------|------|--------|----------------|\n",
    "| Naive pandas | ❌ Crashes | ❌ Exceeds RAM | Simple (but useless) |\n",
    "| Chunked pandas | Slow | ✓ Bounded | Complex (manual) |\n",
    "| DuckDB | Fast | ✓ Efficient | Simple (SQL) |\n",
    "| Polars | Mid | ✓ Efficient | Simple (fluent API) |\n",
    "\n",
    "### When to use what?\n",
    "\n",
    "**DuckDB:**\n",
    "- You know SQL\n",
    "- One-off analytics queries\n",
    "- Joining multiple large files\n",
    "- Working with Parquet files (even faster)\n",
    "\n",
    "**Polars:**\n",
    "- You prefer Python-native APIs\n",
    "- Building data pipelines\n",
    "- Need pandas-like operations but faster\n",
    "\n",
    "**Chunked pandas:**\n",
    "- Legacy code you can't rewrite\n",
    "- Very specific streaming requirements\n",
    "- Otherwise: just don't. Use DuckDB or Polars.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Converting to Parquet\n",
    "\n",
    "If you'll work with this data repeatedly, convert it to Parquet format. Parquet is:\n",
    "- Columnar (only read columns you need)\n",
    "- Compressed (16GB CSV → ~2GB Parquet)\n",
    "- Much faster to read\n",
    "\n",
    "DuckDB can do this conversion without loading the full file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV to Parquet (run once, benefit forever)\n",
    "# It takes a couple of minutes...\n",
    "\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (SELECT * FROM read_csv_auto('../data/cc_news_large.csv'))\n",
    "    TO '../data/cc_news_large.parquet' (FORMAT PARQUET, COMPRESSION ZSTD)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then queries on Parquet are even faster (half the time):\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM '../data/cc_news_large.parquet'\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "    SELECT date\n",
    "    FROM '../data/cc_news_large.parquet'\n",
    "    WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Pandas is not a database.** It loads everything into memory. For files bigger than ~1/3 of your RAM, it will struggle or fail.\n",
    "\n",
    "2. **The \"big data\" threshold is lower than you think.** On a 16GB laptop, a 5GB file is already problematic for pandas.\n",
    "\n",
    "3. **Modern tools make \"hard\" problems trivial.** DuckDB and Polars handle large files elegantly. Learn them.\n",
    "\n",
    "4. **Lazy evaluation is a superpower.** Building a query plan before executing means the tool can optimize. You don't have to.\n",
    "\n",
    "5. **When in doubt, use SQL.** DuckDB lets you write SQL against CSV files. If you know SQL, you already know how to handle big files.\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Exercise\n",
    "\n",
    "Open `02_exercise.ipynb` to apply what you've learned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-ta-massive-text (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
